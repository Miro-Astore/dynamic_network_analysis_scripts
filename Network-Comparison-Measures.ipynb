{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Network Comparison Measures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) D-Measure Distance\n",
    "D-Measure Distance Between Two Graphs - Schieber et al. https://www.nature.com/articles/ncomms13928"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "schieber.py\n",
    "-----------\n",
    "\n",
    "Python implementation of the distance method in 'Quantification of network\n",
    "structural dissimilarities', by Schieber et al.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "def jensen_shannon(dists):\n",
    "    \"\"\"Jensen-Shannong entropy of a family of distributions.\n",
    "\n",
    "    dists is a N by M matrix where each row is the ditribution over a set\n",
    "    of M elements.\n",
    "\n",
    "    \"\"\"\n",
    "    size = dists.shape[0]\n",
    "    vec = np.log(dists.sum(axis=0)) - np.log(size)\n",
    "    first_term = (-1/size) * dists.dot(vec).sum()\n",
    "    second_term = entropy(dists.T).mean()\n",
    "    return first_term - second_term\n",
    "\n",
    "\n",
    "def nnd(graph, dists=None):\n",
    "    \"\"\"Compute Network Node Dispersion (NND).\"\"\"\n",
    "    if dists is None:\n",
    "        dists = node_distance(graph)\n",
    "    diam = dists.shape[1]\n",
    "    return jensen_shannon(dists) / np.log(diam + 1)\n",
    "\n",
    "\n",
    "def node_distance(graph):\n",
    "    \"\"\"All shortest path distances.\n",
    "\n",
    "    The nodes must be labeled by integers from 0 to graph.order() - 1.\n",
    "\n",
    "    \"\"\"\n",
    "    size = graph.order()\n",
    "    if size < 2:\n",
    "        return 1\n",
    "\n",
    "    result = np.zeros((size, size)) # does this need to be sparse?\n",
    "    dists = dict(nx.shortest_path_length(graph))  \n",
    "    dists = np.array([[dists[n1][n2] if dists[n1][n2] < np.inf else size\n",
    "                       for n2 in dists[n1]]\n",
    "                      for n1 in dists])\n",
    "\n",
    "    for idx, row in enumerate(dists):\n",
    "        counts = Counter(row)\n",
    "        result[idx] = [counts[l] for l in range(size)]\n",
    "\n",
    "    diam = (result.sum(axis=0) > 0).sum()\n",
    "    result = result[:, :diam]\n",
    "\n",
    "    return result / size\n",
    "\n",
    "\n",
    "def alpha_centrality(graph, normalize=False):\n",
    "    \"\"\"Bonacich centrality.\"\"\"\n",
    "    size = graph.order()\n",
    "    degrees = graph.degree()\n",
    "    degrees = np.array([degrees[n] for n in graph.nodes()]) / (size - 1)\n",
    "    alpha = 1 / size\n",
    "    exogenous = degrees\n",
    "    mat = sparse.identity(size) - alpha * nx.adjacency_matrix(graph).T\n",
    "    res = sparse.linalg.inv(mat.asformat('csc')).dot(exogenous)\n",
    "    return res if not normalize else res / res.sum()\n",
    "\n",
    "\n",
    "def pad(array, num_cols):\n",
    "    \"\"\"Pad with all-zero columns.\"\"\"\n",
    "    rows = array.shape[0]\n",
    "    cols_to_add = num_cols - array.shape[1]\n",
    "    return np.hstack([array, np.zeros((rows, cols_to_add))])\n",
    "\n",
    "\n",
    "def schieber(graph1, graph2, w1, w2, w3=None, complement=False):\n",
    "    \"\"\"Distance between two graphs. See eqn 2 in the paper.\"\"\"\n",
    "    dists1 = node_distance(graph1)\n",
    "    dists2 = node_distance(graph2)\n",
    "    if dists1.shape[1] > dists2.shape[1]:\n",
    "        pad(dists2, dists1.shape[1])\n",
    "    elif dists2.shape[1] > dists1.shape[1]:\n",
    "        pad(dists1, dists2.shape[1])\n",
    "\n",
    "    first_term = np.vstack([dists1.mean(axis=0), dists2.mean(axis=0)])\n",
    "    first_term = w1 * np.sqrt(jensen_shannon(first_term) / np.log(2))\n",
    "\n",
    "    second_term = np.sqrt(nnd(graph1, dists1)) - np.sqrt(nnd(graph2, dists2))\n",
    "    second_term = w2 * np.abs(second_term)\n",
    "\n",
    "    if w3 is not None:\n",
    "        alpha1 = alpha_centrality(graph1, normalize=True)\n",
    "        alpha2 = alpha_centrality(graph2, normalize=True)\n",
    "        all_alphas = np.vstack([alpha1, alpha2])\n",
    "        third_term = np.sqrt(jensen_shannon(all_alphas) / np.log(2))\n",
    "        if complement:\n",
    "            alpha_comp1 = alpha_centrality(nx.complement(graph1), normalize=True)\n",
    "            alpha_comp2 = alpha_centrality(nx.complement(graph2), normalize=True)\n",
    "            all_alphas = np.vstack([alpha_comp1, alpha_comp2])\n",
    "            third_term += np.sqrt(jensen_shannon(all_alphas) / np.log(2))\n",
    "        third_term = w3 * third_term / 2\n",
    "        return first_term + second_term + third_term\n",
    "    else:\n",
    "        return first_term + second_term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "D=[]\n",
    "for i in range(len(g)):\n",
    "    for j in range(len(g)):\n",
    "        D.append(schieber(g[i],g[j],0.45, 0.45, 0.1,True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) GCM / GCD - Graphlet Comparison\n",
    "GCM and GCD - a method of comparing networks introduced in Yaveroglu et al 2014 - https://www.nature.com/articles/srep04547 and refrenced in Tantardini et al. 2019 - https://www.nature.com/articles/s41598-019-53708-y#Sec2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This first .py file which I have just pasted into a cell generates .gw files \n",
    "\n",
    "#I'm pretty sure this requires python 2 kernel to run - can't remember though. \n",
    "#I may have also run it through terminal using the .py files, and then ran the second set of functions in here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\"\"\"\n",
    "\tThe code to identify the graphlet signatures of all nodes in a network\n",
    "\tusing the ORCA method.\n",
    "\t\n",
    "\tThe script does the formatting of the given LEDA file to the format that \n",
    "\tORCA code can run. Then it reformats the output of the ORCA into ndump2 format.\n",
    "\t\n",
    "\tRun as:\n",
    "\t\t./count.py <LEDA_formatted_network>.gw\n",
    "\t\t\n",
    "\t\tOutputs: <LEDA_formatted_network>.ndump2 in the same folder with the input file\n",
    "\t\n",
    "\tImplemented by:\n",
    "\t\tOmer Nebil Yaveroglu\n",
    "\t\t05.11.2013 - 16:15\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "\"\"\"\n",
    "\tHelper functions are defined here\n",
    "\"\"\"\n",
    "# Read the network in LEDA format\n",
    "# Read LEDA formatted file into a network\n",
    "def readLeda(networkFile):\n",
    "\tnodeList = []\n",
    "\tedgeList = []\n",
    "\t\n",
    "\tfRead = open(networkFile, 'r')\n",
    "\t\n",
    "\tmode = 0\n",
    "\t\n",
    "\tfor line in fRead:\n",
    "\t\tif mode == 0 and line.startswith('|'):\n",
    "\t\t\tmode = 1\n",
    "\t\t\n",
    "\t\tif mode == 1 and line.startswith('|'):\n",
    "\t\t\tnodeList.append(line.strip().strip('|').strip('{').strip('}'))\n",
    "\t\telif mode == 1 and not line.startswith('|'):\n",
    "\t\t\tmode = 2\n",
    "\t\telif mode == 2 and line.strip().endswith('}|'):\n",
    "\t\t\tsplitted = line.strip().split(' ')\n",
    "\t\t\tedgeList.append( [ int(splitted[0]) - 1, int(splitted[1]) - 1] )\n",
    "\t\n",
    "\tfRead.close()\n",
    "\t\n",
    "\treturn (nodeList, edgeList)\n",
    "\n",
    "# Write the network in a format that is ready to get executed by ORCA\n",
    "def writeORCA(edgeList, nodeCount, outputFile):\n",
    "\n",
    "\tfWrite = open(outputFile, 'w')\n",
    "\t\n",
    "\tfWrite.write(str(nodeCount) + ' ' + str(len(edgeList)) + '\\n')\n",
    "\t\n",
    "\tfor edge in edgeList:\n",
    "\t\tfWrite.write(str(edge[0]) + ' ' + str(edge[1]) + '\\n')\n",
    "\n",
    "\tfWrite.close()\n",
    "\n",
    "# Read the temporary ndump2 file and create the original one\n",
    "def formatNdump2(tempNdump2File, originalNdump2, nodeList):\n",
    "\t# Read the temporary result file\n",
    "\tfRead = open(tempNdump2File, 'r')\n",
    "\tfWrite = open(originalNdump2, 'w')\n",
    "\n",
    "\tlineID = 0\n",
    "\tfor line in fRead:\n",
    "\t\tfWrite.write(str(nodeList[lineID]) + ' ' + line)\n",
    "\t\t\n",
    "\t\tlineID += 1\n",
    "\n",
    "\tfRead.close()\t\n",
    "\tfWrite.close()\n",
    "\t\n",
    "\n",
    "\"\"\"\n",
    "\tMain code starts here\n",
    "\"\"\"\n",
    "if __name__ == \"__main__\":\n",
    "\tfor i in xrange(11):\n",
    "\t\tfor j in xrange(60):\n",
    "\t\t\tnetFileName = 'LedaGraph_UnweightedNWs'+str(i+1)+'_'+str(j+1)+'.gw' #CHANGE FILE NAMES HERE\n",
    "\t\n",
    "\t\t\tif not netFileName.endswith('.gw'):\n",
    "\t\t\t\tprint 'ERROR: The network file should be in LEDA format!'\n",
    "\t\t\t\texit(0)\n",
    "\t\n",
    "\t\t# Read the LEDA formatted network\t\n",
    "\t\t\t(nodeList, edgeList) = readLeda(netFileName)\n",
    "\t\n",
    "\t\t# Write in ready to ORCA counting format\n",
    "\t\t\toutputFileName = netFileName.rsplit('.', 1)[0] + '_orca.txt'\n",
    "\t\t\twriteORCA(edgeList, len(nodeList), outputFileName)\n",
    "\t\n",
    "\t\n",
    "\t\t# Run the ORCA graphlet counting code with the resulting file\n",
    "# \t\tset_trace()        \n",
    "\t\t\ttempNdump2File = netFileName.rsplit('.', 1)[0] + '_temp.ndump2'\n",
    "\t\t\tcmd = './orca 5 ' + outputFileName + ' ' + tempNdump2File\n",
    "\t\t\tos.system(cmd)\n",
    "\t\n",
    "\t\t# Format the temp file to original format\n",
    "\t\t\toriginalNdump2 = netFileName.rsplit('.', 1)[0] + '.ndump2'\n",
    "\t\t\tformatNdump2(tempNdump2File, originalNdump2, nodeList)\n",
    "\t\n",
    "\t\t# Counting finished remove the temp network file\n",
    "\t\t\tos.remove(outputFileName)\n",
    "\t\t\tos.remove(tempNdump2File)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This second .py file which I have just pasted into a cell uses the .gw files to calculate network differences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "\"\"\"\n",
    "\tPurpose of the script:\n",
    "\t----------------------\n",
    "\tThe code for computing alignment-free network distances between a set of network given in \n",
    "\ta folder. \n",
    "\t\n",
    "\tThe script automatically searches all relevant network files for the computation of the \n",
    "\tnetwork distances in a given folder. \n",
    "\t\n",
    "\tWith the script, the following network distances can be computed:\t\n",
    "\t\t1) GCD distance \n",
    "\t\t2) RGF Distance\n",
    "\t\t3) GDD-Agreement with Arithmetic Mean (GDD-A)\n",
    "\t\t4) GDD-Agreement with Geometric Mean (GDD-G)\n",
    "\t\t5) Degree Distribution Distance\n",
    "\t\t6) Average Degree Distance\n",
    "\t\t7) Average Clustering Coefficient Distance\n",
    "\t\t8) Diameter Distances of the network\n",
    "\t\n",
    "\tThe script outputs the computed distances between all pairs of networks in CSV format\n",
    "\t\t\n",
    "\tNotes on using the script:\n",
    "\t--------------------------\n",
    "\t\n",
    "\tThe script excepts a folder. Using python's os.walk() function, it automatically \n",
    "\tidentifies all networks under the given folder.\n",
    "\t\n",
    "\tEach network files should contain a set of networks (in LEDA format -- ending \n",
    "\twith extension .gw) and their graphlet degree vector (.ndump2) files.\n",
    "\t\n",
    "\tThe code processes them through the following steps:\n",
    "\t\n",
    "\t1) Reads the networks from '.gw' files and the graphlet signatures from '.ndump' files\n",
    "\t2) Depending on the requested distance measure, computes the relevant network properties \n",
    "\t(e.g., degree distribution, diameter, clustering coefficient) if needed\n",
    "\t3) For computing graphlet degree Compute the log-scaled signatures of all nodes in all files in the folder (log(gd + 1) for all graphlet degrees)\n",
    "\t\n",
    "\tRun as:\n",
    "\t\tpython network_comparison.py <network_folder> <distance_type> <process_count>\n",
    "\t\t\n",
    "\t\t<network_folder> : The folder in which all networks are going to be compared.\n",
    "\t\t\t\tContains the '.ndump2' and '.gw' files of the networks in the cluster\n",
    "\t\t\t\tThe content of the networks folder is as follows:\n",
    "\t\t\t\t\t1) '.ndump2' files - contains the graphlet signatures of all the nodes\n",
    "\t\t\t\t\t2) '.gw' files - contains the network in LEDA format (required only in <test_mode> == 3)\n",
    "\t\t\t\tThe names of the '.gw' files and '.ndump2' files should exactly match.\n",
    "\t\t\n",
    "\t\t<distance_type>: \n",
    "\t\t\t'rgf'\t\t\t- RGF distance\n",
    "\t\t\t'gdda'\t\t\t- GDD Agreement (Both Arithmetic \\& Geometric)\n",
    "\t\t\t'degree' \t\t- Degree distribution & Average degree distances\n",
    "\t\t\t'clustering'\t- Clustering Coefficient\n",
    "\t\t\t'diameter' \t\t- Diameter\n",
    "\t\t\t'gcd11'  \t\t- Graphlet Correlation distance with non-redundant 2-to-4 node graphlet orbits\n",
    "\t\t\t'gcd15'  \t\t- Graphlet Correlation distance with all 2-to-4 node graphlet orbits\n",
    "\t\t\t'gcd58'  \t\t- Graphlet Correlation distance with non-redundant 2-to-5 node graphlet orbits\n",
    "\t\t\t'gcd73'  \t\t- Graphlet Correlation distance with all 2-to-5 node graphlet orbits\n",
    "\t\t\t'spectral'\t\t- Spectral distance using the eigenvalues of the Laplacian representation of the network\n",
    "\t\t\n",
    "\t\t<process_count>:\n",
    "\t\t\tAny number higher than or equal to 1. Determines the number of processes to use for computing the \n",
    "\t\t\tdistances.\n",
    "\t\t\t\n",
    "\tImplemented by:\n",
    "\t\tOmer Nebil Yaveroglu\n",
    "\t\t\n",
    "\t\tFirst implementation \t= 01.06.2012 - 11:53\n",
    "\t\tRevision 1 \t\t\t\t= 10.08.2012 - 16:48\n",
    "\t\tRevision 2 \t\t\t\t= 23.08.2012 - 11:34\n",
    "\t\tParallelized\t\t\t= 09.10.2012 - 17:00\n",
    "\t\tClean Version \t\t\t= 06.05.2014 - 15:21\n",
    "\"\"\"\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import math\n",
    "import numpy\n",
    "import time\n",
    "import networkx as nx\n",
    "import multiprocessing, Queue\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "\"\"\"\n",
    "\tFunctions\n",
    "\"\"\"\n",
    "\n",
    "# Read the signatures from ndump2 files\n",
    "def readSignatures(file):\n",
    "\tsignDict = []\n",
    "\t\n",
    "\tfRead = open(file, 'r')\n",
    "\t\n",
    "\tfor line in fRead:\n",
    "\t\tsignDict.append([float(value) for value in line.strip().split(' ')[-73:]])\n",
    "\t\t\n",
    "\tfRead.close()\n",
    "\t\n",
    "\treturn signDict\n",
    "\n",
    "# Remove the redundant orbits and return the log scaled graphlet degrees\n",
    "def formatSignatures(signList, testMode):\n",
    "\tformattedSignatures = []\n",
    "\t\n",
    "\tfor sign in signList:\n",
    "\t\t# Eliminate the orbits that we are not interested\n",
    "\t\tif testMode == 16:\t\t# GCD-15\n",
    "\t\t\tlog = sign[:15]\n",
    "\t\telif testMode == 14:\t# GCD-73\n",
    "\t\t\tlog = sign\n",
    "\t\telif testMode == 7:\t\t# GCD-58\n",
    "\t\t\teliminateList = [3, 5, 7, 14, 16, 17, 20, 21, 23, 26, 28, 38, 44, 47, 69, 71, 72]\n",
    "\t\t\tlog = [sign[i] for i in range(73) if i not in eliminateList]\n",
    "\t\telif testMode == 10:\t# GCD-11\n",
    "\t\t\teliminateList = [3, 12, 13, 14]\n",
    "\t\t\tlog = [sign[i] for i in range(15) if i not in eliminateList]\n",
    "\t\t\n",
    "\t\tformattedSignatures.append(log)\n",
    "\t\n",
    "\treturn formattedSignatures\n",
    "\n",
    "# Compute the correlation matrix without isnan values by adding a dummy signature\n",
    "def computeCorrelMat(formattedSigns):\n",
    "\t\n",
    "\tlength = len(formattedSigns[0])\n",
    "\t\n",
    "\t# Add the dummy signature for some noise\n",
    "\tformattedSigns.append([1] * length)\n",
    "\t\n",
    "\t# Compute the ranking for the Spearman's correlation coefficient computation\n",
    "\trankList = []\n",
    "\tfor i in range(length):\n",
    "\t\trankList.append(stats.mstats.rankdata([val[i] for val in formattedSigns]))\n",
    "\t\t\n",
    "\tcorrelMat = numpy.corrcoef(rankList, rowvar = 1)\n",
    "\t\t\n",
    "\treturn correlMat\n",
    "\n",
    "\n",
    "# The parallel reading class to compute the orbit correlation matrices depending on the test mode\n",
    "class MatrixReader(multiprocessing.Process):\n",
    "\tdef __init__(self, work_queue, result_queue, testMode):\n",
    "\t\tmultiprocessing.Process.__init__(self)\n",
    "\t\t\n",
    "\t\tself.work_queue \t= work_queue\n",
    "\t\tself.result_queue \t= result_queue\n",
    "\t\tself.testMode \t\t= testMode\n",
    "\t\tself.kill_received \t= False\n",
    "\t\t\n",
    "\t\n",
    "\tdef run(self):\n",
    "\t\twhile not self.kill_received:\n",
    "\t\t\t# Get a task\n",
    "\t\t\ttry:\n",
    "\t\t\t\tndumpName = self.work_queue.get_nowait()\n",
    "\t\t\t\t\n",
    "\t\t\t\tsignatures = readSignatures('{0}.ndump2'.format(ndumpName))\n",
    "\t\t\t\tformatted = formatSignatures(signatures, self.testMode)\t\t\t\t\n",
    "\t\t\t\tcorrelMat = computeCorrelMat(formatted)\n",
    "\t\t\t\t\t\t\t\t\n",
    "\t\t\t\tself.result_queue.put((ndumpName, correlMat))\n",
    "\t\t\t\t\n",
    "\t\t\texcept Queue.Empty:\n",
    "\t\t\t\tpass\n",
    "\t\t\t\n",
    "\n",
    "# Computes the orbit correlation matrices for all the correlation matrices provided in allIndexes \n",
    "def getCorrelationMatrices(allIndexes, testMode):\n",
    "\t# Prepare the list of files to be processed\n",
    "\tfile_queue = multiprocessing.Queue()\n",
    "\tresult_queue = multiprocessing.Queue()\n",
    "\t\n",
    "\tprocessList = []\n",
    "\tfor i in range(num_processes):\n",
    "\t\treader = MatrixReader(file_queue, result_queue, testMode)\n",
    "\t\treader.start()\n",
    "\t\tprocessList.append(reader)\n",
    "\t\n",
    "\t# Put the jobs to be consumed\n",
    "\tjobCount = len(allIndexes)\n",
    "\tsubmitCount = 0\n",
    "\t\n",
    "\tfor index in allIndexes:\n",
    "\t\tfile_queue.put(index)\n",
    "\t\tsubmitCount += 1\n",
    "\t\t\n",
    "\t\t#if submitCount % 100 == 0:\n",
    "\t\t#\tprint 'Submitted correlation computation for: ' , str(float(submitCount) / jobCount * 100) , '%'\n",
    "\n",
    "\t# Process the results of computation\n",
    "\tcorrelMats = {}\n",
    "\t\n",
    "\tfinishedCount = 0\n",
    "\twhile finishedCount < len(allIndexes):\n",
    "\t\ttry:\n",
    "\t\t\tmatrix = result_queue.get_nowait()\n",
    "\t\t\tcorrelMats[matrix[0]] = matrix[1]\n",
    "\t\t\tfinishedCount += 1\n",
    "\t\texcept Queue.Empty:\n",
    "\t\t\ttime.sleep(1)\n",
    "\t\t\n",
    "\t\t#if finishedCount % 100 == 0:\n",
    "\t\t#\tprint 'Finished reading: ', str(float(finishedCount) / jobCount * 100) , '%'\n",
    "\t\n",
    "\tfor proc in processList:\n",
    "\t\tproc.terminate()\n",
    "\t\n",
    "\treturn correlMats\n",
    "\n",
    "# Computes the euclidean distance between two correlation matrices\n",
    "def computeMatrixDist(matrix1, matrix2):\n",
    "\tdifferenceSum = 0\n",
    "\t\n",
    "\tfor i in range(len(matrix1) - 1):\n",
    "\t\tfor j in range( i + 1 , len(matrix1)):\n",
    "\t\t\tdifferenceSum += pow(matrix1[i][j] - matrix2[i][j], 2)\n",
    "\t\n",
    "\teucDist = math.sqrt(differenceSum)\n",
    "\t\n",
    "\treturn eucDist\n",
    "\n",
    "# The parallel reading class to compute the orbit correlation distances\n",
    "class correlDistanceComputer(multiprocessing.Process):\n",
    "\tdef __init__(self, work_queue, result_queue):\n",
    "\t\tmultiprocessing.Process.__init__(self)\n",
    "\t\t\n",
    "\t\tself.work_queue = work_queue\n",
    "\t\tself.result_queue = result_queue\n",
    "\t\tself.kill_received = False\n",
    "\t\n",
    "\tdef run(self):\n",
    "\t\twhile not self.kill_received:\n",
    "\t\t\t# Get a task\n",
    "\t\t\ttry:\n",
    "\t\t\t\t# matrixPair : 0,1 holds names; 2, 3 holds matrices\n",
    "\t\t\t\tmatrixPair = self.work_queue.get_nowait()\n",
    "\t\t\t\tdistance = computeMatrixDist(matrixPair[2], matrixPair[3])\n",
    "\t\t\t\tself.result_queue.put((matrixPair[0], matrixPair[1], distance))\n",
    "\t\t\texcept Queue.Empty:\n",
    "\t\t\t\tpass\n",
    "\n",
    "\n",
    "# Given a matrix, writes the matrix with the network names into the output file\n",
    "def saveDistanceMatrix(matrix, networkNames, outputFile):\n",
    "\tfWrite = open(outputFile, 'w')\n",
    "\t\n",
    "\t# Write the names of the networks\n",
    "\ttoWrite = '\\t'\n",
    "\tfor name in networkNames:\n",
    "\t\ttoWrite += name + '\\t'\n",
    "\tfWrite.write(toWrite.rstrip() + '\\n')\n",
    "\t\n",
    "\t# Write the distances among networks\n",
    "\tfor i in range(len(networkNames)):\n",
    "\t\ttoWrite = networkNames[i] + '\\t'\n",
    "\t\tfor val in matrix[i]:\n",
    "\t\t\ttoWrite += str(val) + '\\t'\n",
    "\t\tfWrite.write(toWrite.rstrip() + '\\n')\n",
    "\t\n",
    "\tfWrite.close()\n",
    "\n",
    "# The function to compute all the distances between the provided correlation matrices in parallel\n",
    "def computeCorrelDist(corrMats, outputName):\n",
    "\t# Start the processes\n",
    "\tpair_queue = multiprocessing.Queue()\n",
    "\tresult_queue = multiprocessing.Queue()\n",
    "\tprocessList = []\n",
    "\t\n",
    "\tfor i in range(num_processes):\n",
    "\t\tcomputer = correlDistanceComputer(pair_queue, result_queue)\n",
    "\t\tcomputer.start()\n",
    "\t\tprocessList.append(computer)\n",
    "\t\n",
    "\t# Put the jobs to be consumed\n",
    "\ttotalJobCount = len(corrMats) * (len(corrMats) - 1) / 2\n",
    "\tmatList = corrMats.keys()\n",
    "\tmatValList = [corrMats[mat] for mat in matList]\n",
    "\t\n",
    "\tpairCount = 0\n",
    "\tfor i in range(len(matValList) - 1):\n",
    "\t\tcorrMat1 = matValList[i]\n",
    "\t\t\n",
    "\t\tfor j in range(i+1, len(matValList)):\n",
    "\t\t\tcorrMat2 = matValList[j]\n",
    "\t\t\t\n",
    "\t\t\tpair_queue.put((i, j, corrMat1, corrMat2))\n",
    "\t\t\tpairCount += 1\n",
    "\t\n",
    "\t# Consume the results of completed computation\n",
    "\tdistances = [[0] * len(corrMats) for i in range(len(corrMats))]\n",
    "\t\n",
    "\tcomputedCount = 0\n",
    "\twhile computedCount < pairCount:\n",
    "\t\ttry:\n",
    "\t\t\tresults = result_queue.get_nowait()\n",
    "\t\t\tdistances[results[0]][results[1]] = results[2] \n",
    "\t\t\tdistances[results[1]][results[0]] = results[2]\n",
    "\t\t\tcomputedCount += 1\n",
    "\t\texcept Queue.Empty:\n",
    "\t\t\ttime.sleep(1)\n",
    "\t\t\n",
    "\tfor proc in processList:\n",
    "\t\tproc.terminate()\n",
    "\t\n",
    "\t# Save the results in the output file\n",
    "\tsaveDistanceMatrix(distances, matList, outputName)\n",
    "\treturn distances \n",
    "# Function to compute the graphlet counts from ndump2 files\n",
    "def getGraphletFreq(signList):\n",
    "\torbits = [2, 3, 5, 7, 8, 9, 12, 14, 17, 18, 23, 25, 27, 33, 34, 35, 39, 44, 45, 50, 52, 55, 56, 61, 62, 65, 69, 70, 72]\n",
    "\tweights = [1, 3, 2, 1, 4, 1, 2, 4, 1, 1, 1, 1, 1, 1, 5, 1, 1, 1, 1, 2, 1, 2, 1, 1, 1, 1, 1, 2, 5]\n",
    "\t\n",
    "\t# Derive the graphlet counts from the orbit degrees\n",
    "\tgraphletCounts = []\n",
    "\t\n",
    "\tfor i in range(len(orbits)):\n",
    "\t\torbit = orbits[i]\n",
    "\t\tsumCount = sum([val[orbit] for val in signList])\n",
    "\t\tgraphletCounts.append(sumCount / weights[i])\n",
    "\t\n",
    "\treturn graphletCounts\n",
    "\n",
    "# Normalize and scale the graphlet distributions for the computation of GDD Agreement\n",
    "def scaleGraphletDists(signatures):\n",
    "\tdistributions = []\n",
    "\t\n",
    "\tfor i in range(73):\n",
    "\t\t# Get the distribution\n",
    "\t\tvalues = {}\n",
    "\t\tfor val in signatures:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tvalues[val[i]] += 1\n",
    "\t\t\texcept:\n",
    "\t\t\t\tvalues[val[i]] = 1\n",
    "\t\n",
    "\t\ttry:\n",
    "\t\t\tdel(values[0])\n",
    "\t\texcept:\n",
    "\t\t\tpass\n",
    "\t\t\n",
    "\t\t# Scale the distribution values for GDD agreement\n",
    "\t\ttotal = 0\n",
    "\t\tfor val in values:\n",
    "\t\t\tvalues[val] = float(values[val]) / val\n",
    "\t\t\ttotal += values[val]\n",
    "\t\n",
    "\t\t# Normalize the distributions\n",
    "\t\tfor val in values:\n",
    "\t\t\tvalues[val] /= total\n",
    "\t\t\t\n",
    "\t\tdistributions.append(values)\n",
    "\t\n",
    "\treturn distributions\n",
    "\n",
    "# Write the distributions for the network\n",
    "def writeDistributions(outputName, distribution):\n",
    "\tfWrite = open(outputName, 'w')\n",
    "\t\n",
    "\ti = 0\n",
    "\t\n",
    "\tfor dictin in distribution:\n",
    "\t\ttoPrint = ''\n",
    "\t\t\n",
    "\t\tfor val in dictin:\n",
    "\t\t\ttoPrint += str(val) + '_' + str(dictin[val]) + ','\n",
    "\t\t\n",
    "\t\tfWrite.write(toPrint.rstrip(',') + '\\n')\n",
    "\t\ti += 1\n",
    "\t\n",
    "\t\n",
    "\tfWrite.close()\n",
    "\n",
    "# The parallel running class for reading the graphlet counts from ndump files\n",
    "class GraphletCountGetter(multiprocessing.Process):\n",
    "\tdef __init__(self, work_queue, result_queue, mode):\n",
    "\t\tmultiprocessing.Process.__init__(self)\n",
    "\t\t\n",
    "\t\tself.work_queue = work_queue\n",
    "\t\tself.result_queue = result_queue\n",
    "\t\tself.mode = mode\n",
    "\t\tself.kill_received = False\n",
    "\t\n",
    "\tdef run(self):\n",
    "\t\twhile not self.kill_received:\n",
    "\t\t\t# Get a task\n",
    "\t\t\ttry:\n",
    "\t\t\t\tndumpName = self.work_queue.get_nowait()\n",
    "\t\t\t\t\n",
    "\t\t\t\tsignatures = readSignatures('{0}.ndump2'.format(ndumpName))\n",
    "\t\t\t\n",
    "\t\t\t\tif self.mode == 1:\n",
    "\t\t\t\t\tcounts = getGraphletFreq(signatures)\n",
    "\t\t\t\t\tself.result_queue.put((ndumpName, counts))\n",
    "\t\t\t\telif self.mode == 2:\n",
    "\t\t\t\t\tdists = scaleGraphletDists(signatures)\n",
    "\t\t\t\t\twriteDistributions(ndumpName, dists)\n",
    "\t\t\t\t\tself.result_queue.put(1)\n",
    "\t\t\texcept:\n",
    "\t\t\t\tpass\n",
    "\t\t\t\n",
    "\t\t\t\n",
    "\n",
    "# The function reads the graphlet signatures in parallel and computes the graphlet counts \n",
    "def getGraphletDists(allIndexes, mode):\n",
    "\t# Start the processes\n",
    "\tfile_queue = multiprocessing.Queue()\n",
    "\tresult_queue = multiprocessing.Queue()\n",
    "\t\n",
    "\tprocessList = []\n",
    "\tfor i in range(num_processes):\n",
    "\t\treader = GraphletCountGetter(file_queue, result_queue, mode)\n",
    "\t\treader.start()\n",
    "\t\tprocessList.append(reader)\n",
    "\t\n",
    "\t# Put the jobs to be consumed\n",
    "\tjobCount = len(allIndexes)\n",
    "\tsubmittedCount = 0\n",
    "\tfor index in allIndexes:\n",
    "\t\tfile_queue.put(index)\n",
    "\t\tsubmittedCount += 1\n",
    "\t\t\n",
    "\t\t#if submittedCount % 100 == 0:\n",
    "\t\t#\tprint 'Distribution Getter Jobs submitted: ' , str(float(submittedCount) / jobCount * 100) , '%'\n",
    "\t\n",
    "\t# Process the results of computation\n",
    "\tgrCounts = {}\n",
    "\t\n",
    "\tfinishedCount = 0\n",
    "\twhile finishedCount < len(allIndexes):\n",
    "\t\ttry:\n",
    "\t\t\tcounts = result_queue.get_nowait()\n",
    "\t\t\tif mode == 1:\n",
    "\t\t\t\tgrCounts[counts[0]] = counts[1]\n",
    "\t\t\tfinishedCount += 1\n",
    "\t\texcept Queue.Empty:\n",
    "\t\t\ttime.sleep(1)\n",
    "\t\t\n",
    "\t\t#if finishedCount % 100 == 0:\n",
    "\t\t#\tprint 'Distributions obtained for:' , str(float(finishedCount) / jobCount * 100) , '%'\n",
    "\t\n",
    "\tfor proc in processList:\n",
    "\t\tproc.terminate()\n",
    "\t\n",
    "\treturn grCounts\n",
    "\n",
    "# Compute the RGF distance among two signatures\n",
    "def computeRGFDist(signs1, signs2):\n",
    "\t# Compute the distance\n",
    "\tT1 = sum(signs1)\n",
    "\tT2 = sum(signs2)\n",
    "\t\n",
    "\tif T1 == 1:\n",
    "\t\tT1 = 1.0000000001\n",
    "\tif T2 == 1:\n",
    "\t\tT2 = 1.0000000001\n",
    "\t\t\n",
    "\tfor i in range(len(signs1)):\n",
    "\t\tif signs1[i] <> 0:\n",
    "\t\t\tsigns1[i] = (-1 * math.log(signs1[i])) / math.log(T1)\n",
    "\t\n",
    "\tfor i in range(len(signs1)):\n",
    "\t\tif signs2[i] <> 0:\n",
    "\t\t\tsigns2[i] = (-1 * math.log(signs2[i])) / math.log(T2)\n",
    "\t\n",
    "\tdistance = 0\n",
    "\tfor i in range(29):\n",
    "\t\tdistance += abs(signs1[i] - signs2[i])\n",
    "\t\n",
    "\treturn distance\n",
    "\n",
    "# Reads the previously saved graphlet degree distribution files\n",
    "def readDist(fileName):\n",
    "\t\n",
    "\tdists = []\n",
    "\t\t\n",
    "\tfRead = open(fileName, 'r')\n",
    "\tfor line in fRead:\n",
    "\t\tdictin = {}\n",
    "\t\t\n",
    "\t\tif line.strip() <> '':\n",
    "\t\t\tline_splitted = line.strip().split(',') \n",
    "\n",
    "\t\t\tfor tuple in line_splitted:\n",
    "\t\t\t\tsplitted = tuple.split('_')\n",
    "\t\t\t\tdictin[float(splitted[0])] = float(splitted[1])\n",
    "\t\t\t\t\n",
    "\t\tdists.append(dictin)\n",
    "\t\t\n",
    "\tfRead.close()\n",
    "\t\n",
    "\treturn dists\n",
    "\n",
    "# Compute the GDD agreement among two networks\n",
    "def computeGDDAgreement(index1, index2):\n",
    "\t# Compute the distributions for each orbit (for both networks)\n",
    "\torbitDist = []\n",
    "\t\t\n",
    "\tsigns1 = readDist(index1)\n",
    "\tsigns2 = readDist(index2)\n",
    "\t\n",
    "\t\n",
    "\tfor i in range(73):\n",
    "\t\tvalues1 = signs1[i]\n",
    "\t\tvalues2 = signs2[i]\n",
    "\t\t\n",
    "\t\t# Compute the distance among the orbits\n",
    "\t\tsumDistances = 0\n",
    "\t\tallDegrees = list(set(values1.keys()) | set(values2.keys()))\n",
    "\t\tfor val in allDegrees:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tscore1 = values1[val]\n",
    "\t\t\texcept:\n",
    "\t\t\t\tscore1 = 0\n",
    "\t\t\t\n",
    "\t\t\ttry:\n",
    "\t\t\t\tscore2 = values2[val]\n",
    "\t\t\texcept:\n",
    "\t\t\t\tscore2 = 0\n",
    "\t\t\t\n",
    "\t\t\tsumDistances += ((score1 - score2)  ** 2)\n",
    "\t\t\n",
    "\t\torbitDist.append(1 - ((1/math.sqrt(2)) * math.sqrt(sumDistances)) )\n",
    "\t\n",
    "\tgdda_distance = numpy.mean(orbitDist)\n",
    "\tgddg_distance = stats.gmean(orbitDist)\n",
    "\t\n",
    "\treturn [gdda_distance, gddg_distance]\n",
    "\n",
    "# The parallel reading class to compute the orbit correlation distances\n",
    "class GraphletDistComputer(multiprocessing.Process):\n",
    "\tdef __init__(self, work_queue, result_queue, mode):\n",
    "\t\tmultiprocessing.Process.__init__(self)\n",
    "\t\t\n",
    "\t\tself.work_queue = work_queue\n",
    "\t\tself.result_queue = result_queue\n",
    "\t\tself.mode = mode\n",
    "\t\tself.kill_received = False\n",
    "\t\t\n",
    "\t\n",
    "\tdef run(self):\n",
    "\t\twhile not self.kill_received:\n",
    "\t\t\t# Get a task\n",
    "\t\t\ttry:\n",
    "\t\t\t\t# matrixPair : 0,1 holds names; 2, 3 holds distributions\n",
    "\t\t\t\tmatrixPair = self.work_queue.get_nowait()\n",
    "\t\t\t\t\t\t\t\t\n",
    "\t\t\t\tif self.mode == 1:\n",
    "\t\t\t\t\trgf = computeRGFDist(matrixPair[2], matrixPair[3])\n",
    "\t\t\t\t\tself.result_queue.put((matrixPair[0], matrixPair[1], rgf))\n",
    "\t\t\t\telif self.mode == 2:\n",
    "\t\t\t\t\t[gdda, gddg] = computeGDDAgreement(matrixPair[2], matrixPair[3])\n",
    "\t\t\t\t\tself.result_queue.put((matrixPair[0], matrixPair[1], 1 - gdda, 1 - gddg))\n",
    "\t\t\texcept:\n",
    "\t\t\t\tpass\n",
    "\t\t\t\n",
    "\n",
    "# Compute the RGF Distances among all pairs of networks\n",
    "def computeRGFDistances(graphletCounts, outputName):\n",
    "\t# Start the processes\n",
    "\tpair_queue = multiprocessing.Queue()\n",
    "\tresult_queue = multiprocessing.Queue()\n",
    "\tprocessList = []\n",
    "\t\n",
    "\tfor i in range(num_processes):\n",
    "\t\tcomputer = GraphletDistComputer(pair_queue, result_queue, 1)\n",
    "\t\tcomputer.start()\n",
    "\t\tprocessList.append(computer)\n",
    "\t\n",
    "\t# Put the jobs to be consumed\n",
    "\tjobCount = len(graphletCounts) * (len(graphletCounts) - 1) / 2\n",
    "\tpairCount = 0\n",
    "\tfor i in range(len(graphletCounts) - 1):\n",
    "\t\tdist1 = graphletCounts.values()[i]\n",
    "\t\t\n",
    "\t\tfor j in range(i+1, len(graphletCounts)):\n",
    "\t\t\tdist2 = graphletCounts.values()[j]\n",
    "\t\t\tpair_queue.put((i, j, dist1, dist2))\n",
    "\t\t\tpairCount += 1\n",
    "\t\t\n",
    "\t\tif pairCount % 1000 == 0:\n",
    "\t\t\tprint 'RGF Comparison Jobs submitted: ' , str(float(pairCount) / jobCount * 100), '%'\n",
    "\t\n",
    "\t# Process the results of computation\n",
    "\tdistances = [[0] * len(graphletCounts) for i in range(len(graphletCounts))]\n",
    "\t\n",
    "\tfinishedCount = 0\n",
    "\twhile finishedCount < pairCount:\n",
    "\t\ttry:\n",
    "\t\t\tresults = result_queue.get_nowait()\n",
    "\t\t\tdistances[results[0]][results[1]] = distances[results[1]][results[0]] = results[2]\n",
    "\t\t\tfinishedCount += 1\n",
    "\t\texcept Queue.Empty:\n",
    "\t\t\ttime.sleep(1)\n",
    "\t\t\n",
    "\t\tif finishedCount % 1000 == 0:\n",
    "\t\t\tprint 'RGF Distance comparisons finished: ', str(float(finishedCount) / jobCount * 100), '%'\n",
    "\t\n",
    "\tfor proc in processList:\n",
    "\t\tproc.terminate()\n",
    "\t\n",
    "\t# Save the results in the output file\n",
    "\tsaveDistanceMatrix(distances, graphletCounts.keys(), outputName)\n",
    "\n",
    "# The process that consumes the computed results to form the final matrix\n",
    "class FinishedConsumer(multiprocessing.Process):\n",
    "\tdef __init__(self, computed_queue, result_queue, jobCount, elementCount):\n",
    "\t\tmultiprocessing.Process.__init__(self)\n",
    "\t\t\n",
    "\t\tself.computed_queue = computed_queue\n",
    "\t\tself.result_queue = result_queue\n",
    "\t\tself.jobCount = jobCount\n",
    "\t\tself.elementCount = elementCount\n",
    "\t\tself.kill_received = False\n",
    "\t\t\n",
    "\t\n",
    "\tdef run(self):\n",
    "\t\tgdda_dists = [[0] * self.elementCount for i in range(self.elementCount)]\n",
    "\t\tgddg_dists = [[0] * self.elementCount for i in range(self.elementCount)]\n",
    "\t\t\t\t\n",
    "\t\tfinishedCount = 0\n",
    "\t\twhile finishedCount < self.jobCount:\n",
    "\t\t\ttry:\n",
    "\t\t\t\tresults = self.computed_queue.get()\n",
    "\t\t\t\tgdda_dists[results[0]][results[1]] = results[2]\n",
    "\t\t\t\tgdda_dists[results[1]][results[0]] = results[2]\n",
    "\t\t\t\tgddg_dists[results[0]][results[1]] = results[3]\n",
    "\t\t\t\tgddg_dists[results[1]][results[0]] = results[3]\n",
    "\t\t\t\tfinishedCount += 1\n",
    "\t\t\texcept Queue.Empty:\n",
    "\t\t\t\tpass\n",
    "\t\t\t\n",
    "\t\t\tif finishedCount % 1000 == 0:\n",
    "\t\t\t\tprint 'Computation completion: ', str(float(finishedCount) / self.jobCount * 100) , '%'\n",
    "\t\t\n",
    "\t\tself.result_queue.put((gdda_dists, gddg_dists))\n",
    "\t\t\n",
    "\n",
    "# Compute the GDD Agreements between all network pairs\n",
    "def computeGDDAgreements(graphletCounts, outputName):\n",
    "\t# Start the processes - as consumers of pair_queue\n",
    "\tpair_queue = multiprocessing.Queue()\n",
    "\tresult_queue = multiprocessing.Queue()\n",
    "\tsummary_queue = multiprocessing.Queue()\n",
    "\tprocessList = []\n",
    "\t\n",
    "\tfor i in range(num_processes):\n",
    "\t\tcomputer = GraphletDistComputer(pair_queue, result_queue, 2)\n",
    "\t\tcomputer.start()\n",
    "\t\tprocessList.append(computer)\n",
    "\t\n",
    "\tjobCount = len(graphletCounts) * (len(graphletCounts) - 1) / 2\n",
    "\tsummarizer = FinishedConsumer(result_queue, summary_queue, jobCount, len(graphletCounts))\n",
    "\tsummarizer.start()\n",
    "\t\n",
    "\t# Put the jobs in the queue\n",
    "\tpairCount = 0\n",
    "\tfor i in range(len(graphletCounts) - 1):\n",
    "\t\tname1 = graphletCounts[i]\n",
    "\t\t\n",
    "\t\tfor j in range(i+1, len(graphletCounts)):\n",
    "\t\t\tname2 = graphletCounts[j]\n",
    "\t\t\t\n",
    "\t\t\tpair_queue.put((i, j, name1, name2))\n",
    "\t\t\tpairCount += 1\n",
    "\t\t\t\t\t\t\n",
    "\t# Wait for the termination of summarizer and get the results from that\n",
    "\tresults = summary_queue.get()\n",
    "\tgdda_dists = results[0]\n",
    "\tgddg_dists = results[1]\n",
    "\t\n",
    "\t# Process the results of computation\n",
    "\tfor proc in processList:\n",
    "\t\tproc.terminate()\n",
    "\t\n",
    "\t# Save the results in the output file\n",
    "\tsaveDistanceMatrix(gdda_dists, graphletCounts, os.path.join(outputName,'gdda.txt'))\n",
    "\tsaveDistanceMatrix(gddg_dists, graphletCounts, os.path.join(outputName,'gddg.txt'))\n",
    "\n",
    "\n",
    "# The function to read a LEDA formatted network file\n",
    "def readLeda(networkFile):\n",
    "\tnetwork = nx.Graph()\n",
    "\t\n",
    "\tfRead = open(networkFile, 'r')\n",
    "\t\n",
    "\tmode = 0\n",
    "\tlistOfNodes = []\n",
    "\t\n",
    "\tfor line in fRead:\n",
    "\t\tline = line.strip()\n",
    "\t\t\n",
    "\t\tif(mode == 0):\n",
    "\t\t\tif line.startswith('|'):\n",
    "\t\t\t\tmode = 1\n",
    "\t\t\n",
    "\t\tif (mode == 1):\n",
    "\t\t\tif line.startswith('|'):\n",
    "\t\t\t\tnodeName = line.strip('|').strip('{').strip('}')\n",
    "\t\t\t\tlistOfNodes.append(nodeName)\n",
    "\t\t\telse:\n",
    "\t\t\t\tmode = 2\n",
    "\t\t\t\tcontinue\n",
    "\t\t\n",
    "\t\tif (mode == 2):\n",
    "\t\t\tsplitted = line.split(' ')\n",
    "\t\t\t\n",
    "\t\t\tnode1 = int(splitted[0]) - 1\n",
    "\t\t\tnode2 = int(splitted[1]) - 1\n",
    "\t\t\t\n",
    "\t\t\tnetwork.add_edge(node1, node2)\n",
    "\t\n",
    "\tfRead.close()\n",
    "\t\n",
    "\treturn network\n",
    "\n",
    "# The parallel reading class for computing the requested network properties\n",
    "class NetworkPropertyGetter(multiprocessing.Process):\n",
    "\tdef __init__(self, work_queue, result_queue, mode):\n",
    "\t\tmultiprocessing.Process.__init__(self)\n",
    "\t\t\n",
    "\t\tself.work_queue = work_queue\n",
    "\t\tself.result_queue = result_queue\n",
    "\t\tself.mode = mode\n",
    "\t\tself.kill_received = False\n",
    "\t\n",
    "\tdef run(self):\n",
    "\t\twhile not self.kill_received:\n",
    "\t\t\t# Get a task\n",
    "\t\t\ttry:\n",
    "\t\t\t\tndumpName = self.work_queue.get_nowait()\n",
    "\t\t\t\t\n",
    "\t\t\t\tnetwork = readLeda(ndumpName + '.gw')\n",
    "\t\t\t\n",
    "\t\t\t\tif self.mode == 4:\n",
    "\t\t\t\t\tprop1\t= nx.degree_histogram(network)\n",
    "\t\t\t\t\tprop2\t= numpy.mean(network.degree().values())\n",
    "\t\t\t\t\tnetProp = (prop1, prop2)\n",
    "\t\t\t\telif self.mode == 5:\n",
    "\t\t\t\t\tnetProp = nx.average_clustering(network)\n",
    "\t\t\t\telif self.mode == 6:\n",
    "\t\t\t\t\tlargestComp = list(nx.connected_component_subgraphs(network))[0]\n",
    "\t\t\t\t\tnetProp = nx.diameter(largestComp)\n",
    "\t\t\t\t\n",
    "\t\t\t\tself.result_queue.put((ndumpName, netProp))\n",
    "\t\t\texcept Queue.Empty:\n",
    "\t\t\t\tpass\n",
    "\t\t\t\n",
    "\t\t\t\n",
    "\n",
    "\n",
    "\t\n",
    "# Given a network file, reads the networks and computes the relevant network properties (that is defined by mode)\n",
    "def computeNetworkProperties(allIndexes, mode):\n",
    "\t\n",
    "\t# Start the processes\n",
    "\tfile_queue = multiprocessing.Queue()\n",
    "\tresult_queue = multiprocessing.Queue()\n",
    "\t\n",
    "\tprocessList = []\n",
    "\tfor i in range(num_processes):\n",
    "\t\treader = NetworkPropertyGetter(file_queue, result_queue, mode)\n",
    "\t\treader.start()\n",
    "\t\tprocessList.append(reader)\n",
    "\t\n",
    "\t# Put the jobs to be consumed\n",
    "\tjobCount = len(allIndexes)\n",
    "\tsubmitCount = 0\n",
    "\tfor index in allIndexes:\n",
    "\t\tfile_queue.put(index)\n",
    "\t\tsubmitCount += 1\n",
    "\t\t\n",
    "\t\tif submitCount % 100 == 0:\n",
    "\t\t\tprint 'Network Property Jobs Submitted:', str(float(submitCount) / jobCount * 100) , '%'\n",
    "\t\t\n",
    "\t\n",
    "\t# Process the results of computation\n",
    "\tproperties = {}\n",
    "\t\n",
    "\tfinishedCount = 0\n",
    "\twhile finishedCount < len(allIndexes):\n",
    "\t\ttry:\n",
    "\t\t\tprops = result_queue.get_nowait()\n",
    "\t\t\tproperties[props[0]] = props[1]\n",
    "\t\t\tfinishedCount += 1\n",
    "\t\texcept Queue.Empty:\n",
    "\t\t\ttime.sleep(1)\n",
    "\t\t\n",
    "\t\tif finishedCount % 100 == 0:\n",
    "\t\t\tprint 'Ready Network Properties:', str(float(finishedCount) / jobCount * 100) , '%'\n",
    "\t\n",
    "\tfor proc in processList:\n",
    "\t\tproc.terminate()\n",
    "\t\n",
    "\treturn properties\n",
    "\t\n",
    "# Evaluate whether the two distributions are sampled from the same continuous distribution\n",
    "def compareDistributions(dist1, dist2):\n",
    "\t# Take the euclidean distance of normalized distributions as in GDD agreement\n",
    "\tnormDist1 = [float(dist1[i]) / i for i in range(1, len(dist1))]\n",
    "\tnormDist2 = [float(dist2[i]) / i for i in range(1, len(dist2))]\n",
    "\tN1 = sum(normDist1)\n",
    "\tN2 = sum(normDist2)\n",
    "\tcompLength = max(len(dist1), len(dist2))\n",
    "\t\n",
    "\tsumDif = 0\n",
    "\tfor i in range(1, compLength):\n",
    "\t\ttry:\n",
    "\t\t\tnorm1 = float(normDist1[i]) / N1\n",
    "\t\texcept:\n",
    "\t\t\tnorm1 = 0\n",
    "\t\t\n",
    "\t\ttry:\n",
    "\t\t\tnorm2 = float(normDist2[i]) / N2\n",
    "\t\texcept:\n",
    "\t\t\tnorm2 = 0\n",
    "\t\t\t\n",
    "\t\tsumDif += ( norm1 - norm2 ) ** 2\n",
    "\t\n",
    "\tdist = math.sqrt(sumDif)\n",
    "\t\n",
    "\treturn dist\n",
    "\n",
    "# The parallel reading class to compute the orbit correlation distances\n",
    "class PropertyComparer(multiprocessing.Process):\n",
    "\tdef __init__(self, work_queue, result_queue, mode):\n",
    "\t\tmultiprocessing.Process.__init__(self)\n",
    "\t\t\n",
    "\t\tself.work_queue = work_queue\n",
    "\t\tself.result_queue = result_queue\n",
    "\t\tself.mode = mode\n",
    "\t\tself.kill_received = False\n",
    "\t\t\n",
    "\t\n",
    "\tdef run(self):\n",
    "\t\twhile not self.kill_received:\n",
    "\t\t\t# Get a task\n",
    "\t\t\ttry:\n",
    "\t\t\t\t# matrixPair : 0,1 holds names; 2, 3 holds distributions\n",
    "\t\t\t\tmatrixPair = self.work_queue.get_nowait()\n",
    "\t\t\t\t\n",
    "\t\t\t\tif self.mode == 1: # Compare degree distribution and average degree\n",
    "\t\t\t\t\tdist1 = compareDistributions(matrixPair[2][0], matrixPair[3][0])\n",
    "\t\t\t\t\tdist2 = math.fabs(matrixPair[2][1] - matrixPair[3][1])\n",
    "\t\t\t\t\tself.result_queue.put((matrixPair[0], matrixPair[1], (dist1, dist2)))\n",
    "\t\t\t\t\t\n",
    "\t\t\t\telif self.mode == 2: # Compare either absolute difference of clustering coef or diameters\n",
    "\t\t\t\t\tdist = math.fabs(matrixPair[2]- matrixPair[3])\n",
    "\t\t\t\t\tself.result_queue.put((matrixPair[0], matrixPair[1], dist))\n",
    "\t\t\t\t\n",
    "\t\t\texcept:\n",
    "\t\t\t\tpass\n",
    "\n",
    "\n",
    "# Given a set of network properties, computes the distance from them\n",
    "def compareNetworkProps(propertyList, distanceMode, outputNames):\n",
    "\t# Start the processes\n",
    "\tpair_queue = multiprocessing.Queue()\n",
    "\tresult_queue = multiprocessing.Queue()\n",
    "\t\n",
    "\tprocessList = []\n",
    "\tfor i in range(num_processes):\n",
    "\t\tcomputer = PropertyComparer(pair_queue, result_queue, distanceMode)\n",
    "\t\tcomputer.start()\n",
    "\t\tprocessList.append(computer)\n",
    "\t\n",
    "\t# Put the jobs to be consumed\n",
    "\tjobCount = len(propertyList) * (len(propertyList) - 1) / 2\n",
    "\tpairCount = 0\n",
    "\tfor i in range(len(propertyList) - 1):\n",
    "\t\tdist1 = propertyList.values()[i]\n",
    "\t\t\n",
    "\t\tfor j in range(i+1, len(propertyList)):\n",
    "\t\t\tdist2 = propertyList.values()[j]\n",
    "\t\t\t\n",
    "\t\t\tpair_queue.put((i, j, dist1, dist2))\n",
    "\t\t\tpairCount += 1\n",
    "\t\t\t\n",
    "\t\t\tif pairCount % 1000 == 0:\n",
    "\t\t\t\tprint 'Jobs submitted:', str(float(pairCount) / jobCount * 100) , '%'\n",
    "\t\n",
    "\t# Process the results of computation\n",
    "\tdistances = [[0] * len(propertyList) for i in range(len(propertyList))]\n",
    "\tif distanceMode == 1:\n",
    "\t\tdists2 = [[0] * len(propertyList) for i in range(len(propertyList))]\n",
    "\t\n",
    "\tfinishedCount = 0 \n",
    "\twhile finishedCount < pairCount:\n",
    "\t\t\n",
    "\t\ttry:\n",
    "\t\t\tresults = result_queue.get_nowait()\n",
    "\t\t\t\n",
    "\t\t\tif distanceMode == 1:\n",
    "\t\t\t\tdistances[results[0]][results[1]] = distances[results[1]][results[0]] = results[2][0]\n",
    "\t\t\t\tdists2[results[0]][results[1]] = dists2[results[1]][results[0]] = results[2][1]\n",
    "\t\t\telse:\n",
    "\t\t\t\tdistances[results[0]][results[1]] = distances[results[1]][results[0]] = results[2]\n",
    "\t\t\tfinishedCount += 1\n",
    "\t\texcept Queue.Empty:\n",
    "\t\t\ttime.sleep(1)\n",
    "\t\t\n",
    "\t\tif finishedCount % 1000 == 0:\n",
    "\t\t\tprint 'Finished comparisons: ' , str(float(finishedCount) / jobCount * 100) ,  '%'\n",
    "\t\t\n",
    "\tfor proc in processList:\n",
    "\t\tproc.terminate()\n",
    "\t\n",
    "\t# Save the results in the output file\n",
    "\tif distanceMode == 1:\n",
    "\t\tsaveDistanceMatrix(distances, propertyList.keys(), outputNames[0])\n",
    "\t\tsaveDistanceMatrix(dists2, propertyList.keys(), outputNames[1])\n",
    "\telse:\n",
    "\t\tsaveDistanceMatrix(distances, propertyList.keys(), outputNames)\n",
    "\t\n",
    "\t\n",
    "# Removes the normalizes distribution files\n",
    "def cleanTempFiles(fileList):\n",
    "\tfor file in fileList:\n",
    "\t\tos.remove(file)\n",
    "\n",
    "\n",
    "# The parallel reading class for computing the requested network properties\n",
    "class SpectrumGetter(multiprocessing.Process):\n",
    "\tdef __init__(self, work_queue, result_queue):\n",
    "\t\tmultiprocessing.Process.__init__(self)\n",
    "\t\t\n",
    "\t\tself.work_queue = work_queue\n",
    "\t\tself.result_queue = result_queue\n",
    "\t\tself.kill_received = False\n",
    "\t\n",
    "\tdef run(self):\n",
    "\t\twhile not self.kill_received:\n",
    "\t\t\t# Get a task\n",
    "\t\t\ttry:\n",
    "\t\t\t\tndumpName = self.work_queue.get_nowait()\n",
    "\t\t\t\t\n",
    "\t\t\t\tnetwork = readLeda('{0}.gw'.format(ndumpName))\n",
    "\t\t\t\tspectrum = sorted(nx.laplacian_spectrum(network), reverse = True)\n",
    "\t\t\t\t\n",
    "\t\t\t\tself.result_queue.put((ndumpName, spectrum))\n",
    "\t\t\texcept Queue.Empty:\n",
    "\t\t\t\tpass\n",
    "\t\t\t\n",
    "\t\n",
    "\t\n",
    "# Given a network file, reads the networks and computes the relevant network properties (that is defined by mode)\n",
    "def getSpectralSignatures(allIndexes):\n",
    "\t\n",
    "\t# Start the processes\n",
    "\tfile_queue = multiprocessing.Queue()\n",
    "\tresult_queue = multiprocessing.Queue()\n",
    "\t\n",
    "\tprocessList = []\n",
    "\tfor i in range(num_processes):\n",
    "\t\treader = SpectrumGetter(file_queue, result_queue)\n",
    "\t\treader.start()\n",
    "\t\tprocessList.append(reader)\n",
    "\t\n",
    "\t# Put the jobs to be consumed\n",
    "\tjobCount = len(allIndexes)\n",
    "\tsubmitCount = 0\n",
    "\tfor index in allIndexes:\n",
    "\t\tfile_queue.put(index)\n",
    "\t\tsubmitCount += 1\n",
    "\t\t\n",
    "\t\tif submitCount % 100 == 0:\n",
    "\t\t\tprint 'Spectrum Computation Jobs Submitted:', str(float(submitCount) / jobCount * 100) , '%'\n",
    "\t\t\n",
    "\t\n",
    "\t# Process the results of computation\n",
    "\tspectrums = {}\n",
    "\t\n",
    "\tfinishedCount = 0\n",
    "\twhile finishedCount < len(allIndexes):\n",
    "\t\ttry:\n",
    "\t\t\tspecs = result_queue.get_nowait()\n",
    "\t\t\tspectrums[specs[0]] = specs[1]\n",
    "\t\t\tfinishedCount += 1\n",
    "\t\texcept Queue.Empty:\n",
    "\t\t\ttime.sleep(1)\n",
    "\t\t\n",
    "\t\tif finishedCount % 100 == 0:\n",
    "\t\t\tprint 'Ready Spectrums:', str(float(finishedCount) / jobCount * 100) , '%'\n",
    "\t\n",
    "\tfor proc in processList:\n",
    "\t\tproc.terminate()\n",
    "\t\n",
    "\treturn spectrums\n",
    "\t\n",
    "# The parallel reading class to compute the orbit correlation distances\n",
    "class SpectrumComparer(multiprocessing.Process):\n",
    "\tdef __init__(self, work_queue, result_queue):\n",
    "\t\tmultiprocessing.Process.__init__(self)\n",
    "\t\t\n",
    "\t\tself.work_queue = work_queue\n",
    "\t\tself.result_queue = result_queue\n",
    "\t\tself.kill_received = False\n",
    "\t\t\n",
    "\t\n",
    "\tdef run(self):\n",
    "\t\twhile not self.kill_received:\n",
    "\t\t\t# Get a task\n",
    "\t\t\ttry:\n",
    "\t\t\t\t# matrixPair : 0,1 holds names; 2, 3 holds distributions\n",
    "\t\t\t\tmatrixPair = self.work_queue.get_nowait()\n",
    "\t\t\t\t\n",
    "\t\t\t\tif len(matrixPair[2]) >= len(matrixPair[3]):\n",
    "\t\t\t\t\tspec1 = matrixPair[2]\n",
    "\t\t\t\t\tspec2 = matrixPair[3]\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\tspec1 = matrixPair[3]\n",
    "\t\t\t\t\tspec2 = matrixPair[2]\n",
    "\t\t\t\t\n",
    "\t\t\t\tspecDistSum = 0\n",
    "\t\t\t\tfor i in range(len(spec1)):\n",
    "\t\t\t\t\tif i < len(spec2):\n",
    "\t\t\t\t\t\tspecDistSum += (spec1[i]-spec2[i]) ** 2\n",
    "\t\t\t\t\telse:\n",
    "\t\t\t\t\t\tspecDistSum += spec1[i] ** 2\n",
    "\t\t\t\t\t\t\n",
    "\t\t\t\tspecDist = math.sqrt(specDistSum)\n",
    "\t\t\t\t\n",
    "\t\t\t\tself.result_queue.put((matrixPair[0], matrixPair[1], specDist))\n",
    "\t\t\t\t\n",
    "\t\t\texcept:\n",
    "\t\t\t\tpass\n",
    "\t\n",
    "\t\n",
    "# Given a set of network properties, computes the distance from them\n",
    "def computeSpectralDists(propertyList, outputName):\n",
    "\t# Start the processes\n",
    "\tpair_queue = multiprocessing.Queue()\n",
    "\tresult_queue = multiprocessing.Queue()\n",
    "\t\n",
    "\tprocessList = []\n",
    "\tfor i in range(num_processes):\n",
    "\t\tcomputer = SpectrumComparer(pair_queue, result_queue)\n",
    "\t\tcomputer.start()\n",
    "\t\tprocessList.append(computer)\n",
    "\t\n",
    "\t# Put the jobs to be consumed\n",
    "\tjobCount = len(propertyList) * (len(propertyList) - 1) / 2\n",
    "\tspectList = propertyList.keys()\n",
    "\tspectValues = [propertyList[key] for key in spectList]\n",
    "\t\n",
    "\tpairCount = 0\n",
    "\tfor i in range(len(propertyList) - 1):\n",
    "\t\tdist1 = spectValues[i]\n",
    "\t\t\n",
    "\t\tfor j in range(i+1, len(propertyList)):\n",
    "\t\t\tdist2 = spectValues[j]\n",
    "\t\t\t\n",
    "\t\t\tpair_queue.put((i, j, dist1, dist2))\n",
    "\t\t\tpairCount += 1\n",
    "\t\t\t\n",
    "\t# Process the results of computation\n",
    "\tdistances = [[0] * len(propertyList) for i in range(len(propertyList))]\n",
    "\t\n",
    "\tfinishedCount = 0 \n",
    "\twhile finishedCount < pairCount:\n",
    "\t\t\n",
    "\t\ttry:\n",
    "\t\t\tresults = result_queue.get_nowait()\n",
    "\t\t\tdistances[results[0]][results[1]] = results[2]\n",
    "\t\t\tdistances[results[1]][results[0]] = results[2]\n",
    "\t\t\tfinishedCount += 1\n",
    "\t\texcept Queue.Empty:\n",
    "\t\t\ttime.sleep(1)\n",
    "\t\t\n",
    "\tfor proc in processList:\n",
    "\t\tproc.terminate()\n",
    "\t\n",
    "\t# Save the results in the output file\n",
    "\tsaveDistanceMatrix(distances, spectList, outputName)\n",
    "\n",
    "# The function to check whether the provided input parameters are meaningful\n",
    "def checkInput(ndumpFolder, distType, num_processes):\n",
    "\tif not os.path.isdir(ndumpFolder):\n",
    "\t\tprint 'ERROR: Provided network folder path is not found: {0}'.format(ndumpFolder)\n",
    "\t\texit(0)\n",
    "\t\n",
    "\tif num_processes < 1:\n",
    "\t\tprint 'ERROR: Enter a correct number for the number of processes...'\n",
    "\t\texit(0)\n",
    "\n",
    "\t# For backwards compatibility with original script, map the distType to corresponding testMode number\n",
    "\ttestMode = None\n",
    "\tif distType == 'rgf':\n",
    "\t\ttestMode = 2\n",
    "\telif distType == 'gdda':\n",
    "\t\ttestMode = 3\n",
    "\telif distType == 'degree':\n",
    "\t\ttestMode = 4\n",
    "\telif distType == 'clustering':\n",
    "\t\ttestMode = 5\n",
    "\telif distType == 'diameter':\n",
    "\t\ttestMode = 6\n",
    "\telif distType == 'gcd58':\n",
    "\t\ttestMode = 7\n",
    "\telif distType == 'gcd11':\n",
    "\t\ttestMode = 10\n",
    "\telif distType == 'gcd73':\n",
    "\t\ttestMode = 14\n",
    "\telif distType == 'gcd15':\n",
    "\t\ttestMode = 16\n",
    "\telif distType == 'spectral':\n",
    "\t\ttestMode = 17\n",
    "\telse:\n",
    "\t\tprint 'ERROR: Unknown network distance type: {0}'.format(distType)\n",
    "\t\tprint 'Available options are:'\n",
    "\t\tprint 'rgf\t\t\t- RGF distance'\n",
    "\t\tprint 'gdda\t\t\t- GDD Agreement (Both Arithmetic \\& Geometric)'\n",
    "\t\tprint 'degree \t\t- Degree distribution & Average degree distances'\n",
    "\t\tprint 'clustering\t- Clustering Coefficient'\n",
    "\t\tprint 'diameter \t- Diameter'\n",
    "\t\tprint 'gcd11  \t\t- Graphlet Correlation distance with non-redundant 2-to-4 node graphlet orbits'\n",
    "\t\tprint 'gcd15  \t\t- Graphlet Correlation distance with all 2-to-4 node graphlet orbits'\n",
    "\t\tprint 'gcd58  \t\t- Graphlet Correlation distance with non-redundant 2-to-5 node graphlet orbits'\n",
    "\t\tprint 'gcd73  \t\t- Graphlet Correlation distance with all 2-to-5 node graphlet orbits'\n",
    "\t\tprint 'spectral\t\t- Spectral distance using the eigenvalues of the Laplacian representation of the network'\n",
    "\t\texit(0)\n",
    "\t\n",
    "\treturn testMode\n",
    "\n",
    "# The function to search the given folder for ndump2 files\n",
    "def searchFolder(ndumpFolder, testMode):\n",
    "\n",
    "\t# Identify all files under the folder\n",
    "\tdirectory \t= os.walk(ndumpFolder)\n",
    "\t\n",
    "\t# Pick the relevant files based on ndump2 extensions\n",
    "\tallIndexes\t= []\n",
    "\tfor file in directory:\n",
    "\t\tpath = file[0]\n",
    "\t\t\n",
    "\t\tfor fileName in file[2]:\n",
    "\t\t\tif fileName.endswith('.ndump2'):\n",
    "\t\t\t\tstripName = fileName.rsplit('.', 1)[0]\n",
    "\t\t\t\tindexName = os.path.join(path, stripName)\n",
    "\t\t\t\t\n",
    "\t\t\t\t# Read networks for tests 4, 5, 6\n",
    "\t\t\t\tif testMode in [4, 5 ,6, 17]  and stripName + '.gw' not in file[2]:\n",
    "\t\t\t\t\tprint 'ERROR: Network file - Ndump file mismatch!'\n",
    "\t\t\t\t\tprint 'Check: {0}.gw'.format(indexName)\n",
    "\t\t\t\t\texit(0)\n",
    "\t\t\t\t\n",
    "\t\t\t\tallIndexes.append(indexName)\n",
    "\t\t\t\t\n",
    "\treturn allIndexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This is how to run the GCD code above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process the program parameters\n",
    "ndumpFolder \t= fullFolderPath #folder\n",
    "distType\t\t= 'gcd11' #change type based on documentation above\n",
    "num_processes\t= int(5)\n",
    "\n",
    "# Due to backward compatibility, map the distance type to corresponding testMode\n",
    "testMode  \t\t= checkInput(ndumpFolder, distType, num_processes) \n",
    "\n",
    "# Identify the files based on the 'ndump2' extensions\t\n",
    "allIndexes  \t= searchFolder(ndumpFolder, testMode)\n",
    "\n",
    "\n",
    "# Compute the graphlet correlation distance (whatever type it is)\n",
    "if testMode in [7, 10, 14, 16]:\n",
    "    corrMats = getCorrelationMatrices(allIndexes, testMode) #genereates correlation matrices\n",
    "    print 'GCMs are ready! Computing the GCDs...'\t\t\n",
    "    distances=computeCorrelDist(corrMats, os.path.join(ndumpFolder, '{0}.txt'.format(distType))) #computes distances\n",
    "    print 'Finished'\n",
    "#         # Compute the spectral distances\n",
    "# \telif testMode == 17:\n",
    "# \t\t# Compute the spectral distance of laplacian\n",
    "# \t\tspectralSigns = getSpectralSignatures(allIndexes)\n",
    "# \t\tprint 'Spectral Signatures are ready! Computing the distances...'\n",
    "# \t\tcomputeSpectralDists(spectralSigns, os.path.join(ndumpFolder, 'spectralDist.txt'))\n",
    "\t\t\n",
    "# \t# Compute the RGF Distance\n",
    "# \telif testMode == 2:\n",
    "# \t\tgraphletCounts = getGraphletDists(allIndexes, 1)\n",
    "# \t\tprint 'Graphlet Counts ready! Computing the distances...'\n",
    "# \t\tcomputeRGFDistances(graphletCounts, os.path.join(ndumpFolder, 'rgf.txt'))\n",
    "\t\t\n",
    "# \t# Compute the GDD-Agreement with Arithmetics and Geometric mean\n",
    "# \telif testMode == 3:\n",
    "# \t\tgetGraphletDists(allIndexes, 2)\n",
    "# \t\tprint 'Graphlet Distributions ready! Computing the distances...'\n",
    "# \t\tcomputeGDDAgreements(allIndexes, ndumpFolder)\n",
    "# \t\tcleanTempFiles(allIndexes)\n",
    "\t\t\n",
    "# \t# Compute the degree based distance measures\n",
    "# \telif testMode == 4:\n",
    "# \t\tdegreeDists = computeNetworkProperties(allIndexes, 4)\n",
    "# \t\tprint 'Degree Distributions ready! Computing the distances...'\n",
    "# \t\tcompareNetworkProps(degreeDists, 1, [os.path.join(ndumpFolder, 'degree_dists.txt') , os.path.join(ndumpFolder, 'av_degree.txt')])\n",
    "\t\t\n",
    "# \t# Compute the clustering coefficient based distance measures\n",
    "# \telif testMode == 5:\n",
    "# \t\tclustCoefs = computeNetworkProperties(allIndexes, 5)\n",
    "# \t\tprint 'Clustering Coefficients ready! Computing the distances...'\n",
    "# \t\tcompareNetworkProps(clustCoefs, 2, os.path.join(ndumpFolder, 'clust_coef.txt'))\n",
    "\t\t\n",
    "# \t# Compute the diameter based distance measures\n",
    "# \telif testMode == 6:\n",
    "# \t\tdiameters = computeNetworkProperties(allIndexes, 6)\n",
    "# \t\tprint 'Diameters ready! Computing the distances...'\n",
    "# \t\tcompareNetworkProps(diameters, 2, os.path.join(ndumpFolder, 'diameter.txt'))\n",
    "\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Portrait Divergence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "# portrait_divergence.py\n",
    "# Jim Bagrow\n",
    "# Last Modified: 2018-04-24\n",
    "\n",
    "import sys, os\n",
    "import tempfile\n",
    "import argparse\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from scipy.stats import entropy\n",
    "\n",
    "\n",
    "def portrait_cpp(graph, fname=None, keepfile=False):\n",
    "    \"\"\"Compute and generate portrait of graph using compiled B_matrix\n",
    "    executable.\n",
    "    \n",
    "    Return matrix B where B[i,j] is the number of starting nodes in graph with\n",
    "    j nodes in shell i\n",
    "    \"\"\"\n",
    "    # file to save to:\n",
    "    f = fname\n",
    "    if fname is None:\n",
    "        f = next(tempfile._get_candidate_names())\n",
    "    \n",
    "    # make sure nodes are 0,...,N-1 integers:\n",
    "    graph = nx.convert_node_labels_to_integers(graph)\n",
    "    \n",
    "    # write edgelist:\n",
    "    nx.write_edgelist(graph, f+\".edgelist\", data=False)\n",
    "    \n",
    "    # make B-matrix:\n",
    "    os.system(\"./B_matrix {}.edgelist {}.Bmat > /dev/null\".format(f, f))\n",
    "    portrait = np.loadtxt(\"{}.Bmat\".format(f))\n",
    "    \n",
    "    # clean up:\n",
    "    if not keepfile:\n",
    "        os.remove(f+\".edgelist\")\n",
    "        os.remove(f+\".Bmat\")\n",
    "    \n",
    "    return portrait\n",
    "\n",
    "\n",
    "def portrait_py(graph):\n",
    "    \"\"\"Return matrix B where B[i,j] is the number of starting nodes in graph\n",
    "    with j nodes in shell i.\n",
    "    \n",
    "    If this function is too slow, consider portrait_cpp() instead.\n",
    "    \"\"\"\n",
    "    dia = 500 #nx.diameter(graph)\n",
    "    N = graph.number_of_nodes()\n",
    "    # B indices are 0...dia x 0...N-1:\n",
    "    B = np.zeros((dia+1,N)) \n",
    "    \n",
    "    max_path = 1\n",
    "    adj = graph.adj\n",
    "    for starting_node in graph.nodes():\n",
    "        nodes_visited = {starting_node:0}\n",
    "        search_queue = [starting_node]\n",
    "        d = 1\n",
    "        while search_queue:\n",
    "            next_depth = []\n",
    "            extend = next_depth.extend\n",
    "            for n in search_queue:\n",
    "                l = [i for i in adj[n] if i not in nodes_visited] \n",
    "                extend(l)\n",
    "                for j in l:\n",
    "                    nodes_visited[j] = d\n",
    "            search_queue = next_depth\n",
    "            d += 1\n",
    "            \n",
    "        node_distances = nodes_visited.values()\n",
    "        max_node_distances = max(node_distances)\n",
    "        \n",
    "        curr_max_path = max_node_distances\n",
    "        if curr_max_path > max_path:\n",
    "            max_path = curr_max_path\n",
    "        \n",
    "        # build individual distribution:\n",
    "        dict_distribution = dict.fromkeys(node_distances, 0)\n",
    "        for d in node_distances:\n",
    "            dict_distribution[d] += 1\n",
    "        # add individual distribution to matrix:\n",
    "        for shell,count in dict_distribution.items():\n",
    "            B[shell][count] += 1\n",
    "        \n",
    "        # HACK: count starting nodes that have zero nodes in farther shells\n",
    "        max_shell = dia\n",
    "        while max_shell > max_node_distances:\n",
    "            B[max_shell][0] += 1\n",
    "            max_shell -= 1\n",
    "    \n",
    "    return B[:max_path+1,:]\n",
    "\n",
    "\n",
    "portrait = portrait_py\n",
    "#portrait = portrait_cpp\n",
    "\n",
    "\n",
    "def weighted_portrait(G, paths=None, binedges=None):\n",
    "    \"\"\"Compute weighted portrait of G, using Dijkstra's algorithm for finding\n",
    "    shortest paths. G is a networkx object.\n",
    "    \n",
    "    Return matrix B where B[i,j] is the number of starting nodes in graph with\n",
    "    j nodes at distance d_i <  d < d_{i+1}.\n",
    "    \"\"\"\n",
    "    # all pairs path lengths\n",
    "    if paths is None:\n",
    "        paths = list(nx.all_pairs_dijkstra_path_length(G))\n",
    "    \n",
    "    if binedges is None:\n",
    "        unique_path_lengths  = _get_unique_path_lengths(G, paths=paths)\n",
    "        sampled_path_lengths = np.percentile(unique_path_lengths, np.arange(0, 101, 1))\n",
    "    else:\n",
    "        sampled_path_lengths = binedges\n",
    "    UPL = np.array(sampled_path_lengths)\n",
    "    \n",
    "    l_s_v = []\n",
    "    for i,(s,dist_dict) in enumerate(paths):\n",
    "        distances = np.array(list(dist_dict.values()))\n",
    "        s_v,e = np.histogram(distances, bins=UPL)\n",
    "        l_s_v.append(s_v)\n",
    "    M = np.array(l_s_v)\n",
    "    \n",
    "    B = np.zeros((len(UPL)-1, G.number_of_nodes()+1))\n",
    "    for i in range(len(UPL)-1):\n",
    "        col = M[:,i] # ith col = numbers of nodes at d_i <= distance < d_i+1\n",
    "        for n,c in Counter(col).items():\n",
    "            B[i,n] += c\n",
    "    \n",
    "    return B\n",
    "\n",
    "\n",
    "def _get_unique_path_lengths(graph, paths=None):\n",
    "    if paths is None:\n",
    "        paths = list(nx.all_pairs_dijkstra_path_length(graph))\n",
    "\n",
    "    unique_path_lengths = set()\n",
    "    for starting_node,dist_dict in paths:\n",
    "        unique_path_lengths |= set(dist_dict.values())\n",
    "    unique_path_lengths = sorted(list(unique_path_lengths))\n",
    "    return unique_path_lengths\n",
    "\n",
    "\n",
    "def pad_portraits_to_same_size(B1,B2):\n",
    "    \"\"\"Make sure that two matrices are padded with zeros and/or trimmed of\n",
    "    zeros to be the same dimensions.\n",
    "    \"\"\"\n",
    "    ns,ms = B1.shape\n",
    "    nl,ml = B2.shape\n",
    "    \n",
    "    # Bmats have N columns, find last *occupied* column and trim both down:\n",
    "    lastcol1 = max(np.nonzero(B1)[1])\n",
    "    lastcol2 = max(np.nonzero(B2)[1])\n",
    "    lastcol = max(lastcol1,lastcol2)\n",
    "    B1 = B1[:,:lastcol+1]\n",
    "    B2 = B2[:,:lastcol+1]\n",
    "    \n",
    "    BigB1 = np.zeros((max(ns,nl), lastcol+1))\n",
    "    BigB2 = np.zeros((max(ns,nl), lastcol+1))\n",
    "    \n",
    "    BigB1[:B1.shape[0],:B1.shape[1]] = B1\n",
    "    BigB2[:B2.shape[0],:B2.shape[1]] = B2\n",
    "    \n",
    "    return BigB1, BigB2\n",
    "\n",
    "\n",
    "def _graph_or_portrait(X):\n",
    "    \"\"\"Check if X is a nx (di)graph. If it is, get its portrait. Otherwise\n",
    "    assume it's a portrait and just return it.\n",
    "    \"\"\"\n",
    "    if isinstance(X, (nx.Graph, nx.DiGraph)):\n",
    "        return portrait(X)\n",
    "    return X\n",
    "\n",
    "\n",
    "def portrait_divergence(G, H):\n",
    "    \"\"\"Compute the network portrait divergence between graphs G and H.\"\"\"\n",
    "    \n",
    "    BG = _graph_or_portrait(G)\n",
    "    BH = _graph_or_portrait(H)\n",
    "    BG, BH = pad_portraits_to_same_size(BG,BH)\n",
    "    \n",
    "    L, K = BG.shape\n",
    "    V = np.tile(np.arange(K),(L,1))\n",
    "    \n",
    "    XG = BG*V / (BG*V).sum()\n",
    "    XH = BH*V / (BH*V).sum()\n",
    "    \n",
    "    # flatten distribution matrices as arrays:\n",
    "    P = XG.ravel()\n",
    "    Q = XH.ravel()\n",
    "    \n",
    "    # lastly, get JSD:\n",
    "    M = 0.5*(P+Q)\n",
    "    KLDpm = entropy(P, M, base=2)\n",
    "    KLDqm = entropy(Q, M, base=2)\n",
    "    JSDpq = 0.5*(KLDpm + KLDqm)\n",
    "    \n",
    "    return JSDpq\n",
    "\n",
    "\n",
    "def portrait_divergence_weighted(G,H, bins=None, binedges=None):\n",
    "    \"\"\"Network portrait divergence between two weighted graphs.\n",
    "    \n",
    "    bins = width of bins in percentiles\n",
    "    binedges = vector of bin edges\n",
    "    bins and binedges are mutually exclusive\n",
    "    \"\"\"\n",
    "    \n",
    "    # get joint binning:\n",
    "    paths_G = list(nx.all_pairs_dijkstra_path_length(G))\n",
    "    paths_H = list(nx.all_pairs_dijkstra_path_length(H))\n",
    "    \n",
    "    # get bin_edges in common for G and H:\n",
    "    if binedges is None:\n",
    "        if bins is None:\n",
    "            bins = 1\n",
    "        UPL_G = set(_get_unique_path_lengths(G, paths=paths_G))\n",
    "        UPL_H = set(_get_unique_path_lengths(H, paths=paths_H))\n",
    "        unique_path_lengths = sorted(list(UPL_G | UPL_H))\n",
    "        binedges = np.percentile(unique_path_lengths, np.arange(0, 101, bins))\n",
    "    \n",
    "    # get weighted portraits:\n",
    "    BG = weighted_portrait(G, paths=paths_G, binedges=binedges)\n",
    "    BH = weighted_portrait(H, paths=paths_H, binedges=binedges)\n",
    "    \n",
    "    return portrait_divergence(BG, BH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#To run:\n",
    "# portrait_divergence(Graph1,Graph2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
